{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBGVFHfgRfU5ipuk/DOJqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LennyRBriones/pytorch/blob/main/creating_a_model_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, nn.Module is very important. being the class that allows you create, store and manipulate all layers and operations in the neural networks.\n",
        "\n",
        "This is very important because with the nn.module you only need to keep focus in the model building and results, the layers will be attended by the nn.module for you\n",
        "\n"
      ],
      "metadata": {
        "id": "dLZbe1faHBJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a7PCWmBXGcXJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "## type of model and module to use\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "                    #size, dimentions, hidden layer, output layer\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    #first the embedding  from the library, our size and the dimention\n",
        "\n",
        "    self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "    #recurrent neural network, emedding, hidden layer, num of layers, processing batcher one by one\n",
        "\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    #forward conecction to lineal layer having the hidden and output\n",
        "\n",
        "  def fordwar(self, text):\n",
        "    #always forwdard class, in this case recieving text\n",
        "\n",
        "    embedded = self.embedding(text)\n",
        "    #generating embedding\n",
        "\n",
        "    output, (hidden, cell) = self.rnn(embedded)\n",
        "    #conexion between hidden and embeed\n",
        "\n",
        "    final_hidden = hidden[-1]\n",
        "    #getting the final versión of hidden layer\n",
        "\n",
        "    return self.fc(final_hidden)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reviewing code Step by step\n",
        "\n",
        "\n",
        "\n",
        "1.   Definned a class called TextClassifier taking data from nn.Module\n",
        "2.   Build the layers in our model: one layer to embedding, one layer to LSTM using 2 layers and hidden_dim hidden layers for each layer, finally a lineal layer as output\n",
        "3.  Using the method forward we created an embedding with inpout text using the embedding layer\n",
        "4.   Then through LSTM layer using the embedding we get the final hidden layer\n",
        "5.   Finally, using the final hidden layer through the lineal layer we get the output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uIbtUR5cj-Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to use the model you need to instance it like this:"
      ],
      "metadata": {
        "id": "2i3Z0_-ImdqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000 # its an hyperparameter, it could be even 60,000\n",
        "embedding_dim = 100 # even 700\n",
        "hidden_dim = 256 #as conventional use\n",
        "output_dim = 2 # because it´s a classification model\n",
        "\n",
        "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim )"
      ],
      "metadata": {
        "id": "5UGgqdX3JG4N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see the structure of our model"
      ],
      "metadata": {
        "id": "oh3JzHKbn6Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLLDO_En4Gl",
        "outputId": "2841bbc7-18af-4212-ff3c-8b8aad8c7187"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextClassifier(\n",
              "  (embedding): Embedding(1000, 100)\n",
              "  (rnn): LSTM(100, 256, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}