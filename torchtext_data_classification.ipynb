{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqyjUnWzwM/wf8RaYuwt2m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LennyRBriones/pytorch/blob/main/torchtext_data_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library & dataset\n"
      ],
      "metadata": {
        "id": "CVLScX7JIZ_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sl1w13qpH7Zd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install portalocker>=2.0.0\n",
        "!pip install torchtext --updgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import DBpedia\n",
        "\n",
        "# version\n",
        "torchtext.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mzyLwyQGI6bH",
        "outputId": "340d3d9f-fc96-4a99-9c64-2205dfd2f1f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.15.2+cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the dataset and starting the vocabulary"
      ],
      "metadata": {
        "id": "FraBAdnkJpm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = iter(DBpedia(split=\"train\"))"
      ],
      "metadata": {
        "id": "1tSBWSDDJHBr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(train_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIliz0aZJ3Fa",
        "outputId": "2c3cfffa-4d34-488f-9040-007e49947d72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'E. D. Abbott Ltd  Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = DBpedia(split=\"train\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n"
      ],
      "metadata": {
        "id": "pC_WwWaDJ6OX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our vocabulary transforms the list of tokens in int numbers"
      ],
      "metadata": {
        "id": "t6hBXFmQNI4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Hi there!, here Lenny making tests\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGlkzlj8M8x8",
        "outputId": "16fa8504-64c0-4b14-de2d-8dd0b73f8847"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'there', '!', ',', 'here', 'lenny', 'making', 'tests']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(tokenizer(\"Hi there!, here Lenny making tests, nihao!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teiuPcTRNUvh",
        "outputId": "0b109df4-f40e-4044-fac8-64ffcbe1ad6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10371, 313, 403, 90515, 1538, 13823, 1031, 5247, 90515, 0, 403]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer classify every word that is register, in this case tnakns to `<unk> ` the words that are not register as \"nihao\" is store as 0"
      ],
      "metadata": {
        "id": "3ZibMFbpOg-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) -1\n",
        "#to save starting in 0"
      ],
      "metadata": {
        "id": "u7UGiaFINZxR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline(\"Hi, I'am Lenny\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktTVVhb6PKTD",
        "outputId": "6778d957-efa1-477d-c619-f153f7c36431"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10371, 90515, 187, 17, 2409, 13823]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_pipeline(\"10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDgwEy3RPQPX",
        "outputId": "e71255d0-814e-4a01-b49c-63e5c18bb5ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Dataloader` allows to load big data in a small batchers"
      ],
      "metadata": {
        "id": "wRzmIMXyPypN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using cuda to big data process\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "  label_list = []\n",
        "  text_list = []\n",
        "  offsets = [0]\n",
        "\n",
        "  for (_label, _text) in batch:\n",
        "    label_list.append(label_pipeline(_label))\n",
        "    processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "    offsets.append(processed_text.size(0))\n",
        "\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text_list = torch.cat(text_list)\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "WXrvA56XP-cu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_iter = DBpedia(split=\"train\")\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch )\n",
        "                          # 8 in this case using a colab CPU"
      ],
      "metadata": {
        "id": "w_mUkTxePVu6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9BY2CaATOAL",
        "outputId": "99aff873-2d00-4c59-8bdf-ee01f60020cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x79c8c8932500>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Classificationtextmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "   super(Classificationtextmodel, self).__init__()\n",
        "\n",
        "      # Embedding layer\n",
        "   self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
        "\n",
        "      # Batch Normalization\n",
        "   self.bn1 = nn.BatchNorm1d(embed_dim)\n",
        "\n",
        "      # Fully conenected layer\n",
        "\n",
        "   self.fc = nn.Linear(embed_dim, num_class)\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "      # Embed the text\n",
        "    embedded = self.embedding(text, offsets)\n",
        "\n",
        "      # Apply Batch normalization\n",
        "    embedded_norm = self.bn1(embedded)\n",
        "\n",
        "      # Apply the ReLU activation function\n",
        "    embedded_activated = F.relu(embedded_norm)\n",
        "\n",
        "      # Output the class probabilities\n",
        "    return self.fc(embedded_activated)\n"
      ],
      "metadata": {
        "id": "LEMPD-abTWVX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the model with embeddin of 100"
      ],
      "metadata": {
        "id": "YOXRO3cqtoyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = DBpedia(split=\"train\")\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 100\n",
        "\n",
        "model = Classificationtextmodel(vocab_size=vocab_size, embed_dim=embedding_size, num_class=num_class).to(device)"
      ],
      "metadata": {
        "id": "LS3N6zgqtWm4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5SCdzlYwtB-",
        "outputId": "dffc947d-d22f-498e-9983-49b7b0208c73"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "802998"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model architecture\n",
        "print(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxCMgtFqw0rq",
        "outputId": "cb4a25e8-2d29-4900-a08c-aec62a213fc0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classificationtextmodel(\n",
            "  (embedding): EmbeddingBag(802998, 100, mode='mean')\n",
            "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc): Linear(in_features=100, out_features=14, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of trainable parameters in our model\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "          #pnumel to get parameters                          #gradient = trainable\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0Ml9POHw952",
        "outputId": "7b83240d-ebed-4d2a-954b-12d78265add9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 80,301,414 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluate in our Model"
      ],
      "metadata": {
        "id": "w6K0cShF1yDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(dataloader):\n",
        "    # Turns the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # initialize the accuracy, count & loss for every epoch\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "      # Reestar gradient every batch\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Get model predictions\n",
        "      prediction = model(text, offsets)\n",
        "\n",
        "      #Get the loss\n",
        "      loss = criteria(prediction, label)\n",
        "\n",
        "      #backpropage the loss and get the gradients\n",
        "      loss.backward()\n",
        "\n",
        "      #Get the accuracy\n",
        "\n",
        "      acc = (prediction.argmax(1) == label).sum()\n",
        "\n",
        "      #Avoid the gradients elevated it´s values\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "      # Update the wheights\n",
        "      optimizer.step()\n",
        "\n",
        "      # Get the sum of loss & the accuracy for every epoch\n",
        "      epoch_acc += acc.item()\n",
        "      epoch_loss += loss.item()\n",
        "      total_count += label.size(0)\n",
        "\n",
        "      if idx % 500 == 0 and idx > 0:\n",
        "        print(f\" epoch {epoch} | {idx}/{len(dataloader)} batches | loss {epoch_loss/total_count} | accuracy {epoch_acc/total_count}\")\n",
        "\n",
        "\n",
        "    return epoch_acc/total_count, epoch_loss/total_count\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iPD6qq6wxeZ6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader):\n",
        "  model.eval()\n",
        "  epoch_acc = 0\n",
        "  total_count = 0\n",
        "  epoch_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "              # Get the predicted label\n",
        "      prediction = model(text, offsets)\n",
        "\n",
        "              # Get loss & accuracy\n",
        "      loss = criteria(prediction, label)\n",
        "      acc = (prediction.argmax(1) == label).sum()\n",
        "\n",
        "              # Get the new sum of loss & accuracy\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "      total_count += label.size(0)\n",
        "\n",
        "  return epoch_acc/total_count, epoch_loss/total_count\n",
        "\n"
      ],
      "metadata": {
        "id": "MNllYc6zHD3F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training: Data Split, loss & Optimization"
      ],
      "metadata": {
        "id": "T6M74PUYJpWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperpatameters\n",
        "\n",
        "epochs = 3\n",
        "learning_rate = 0.2\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "u50tfnn1JVim"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss, optimizer\n",
        "\n",
        "criteria = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
      ],
      "metadata": {
        "id": "aMNQ-nzPJ-ri"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split of Data"
      ],
      "metadata": {
        "id": "xTPqOCP_T5O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "# Get the trainset & testset\n",
        "train_iter, test_iter = DBpedia()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Train the model using the 95% of data from trainset\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "# Get a validation dataset with 5% of trainset\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset)-num_train])\n",
        "\n",
        "# Get dataloader ready to upload to our model\n",
        "train_dataloader = DataLoader(split_train_, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "sBSeuXq5Ka_0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Evaluation"
      ],
      "metadata": {
        "id": "Wi-FED-HWxp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best loss value\n",
        "major_loss_validation = float(\"inf\")\n",
        "\n",
        "# Training\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Train\n",
        "    training_acc, training_loss = training(train_dataloader)\n",
        "\n",
        "    #Validation\n",
        "    validation_acc, validation_loss = evaluate(valid_dataloader)\n",
        "\n",
        "    #Save the best model\n",
        "    if validation_loss < major_loss_validation:\n",
        "      best_valid_loss = validation_loss\n",
        "      torch.save(model.state_dict(), \"best_models.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8VyGTtAWiel",
        "outputId": "0078d571-4351-4eb7-b38d-5cc0a3892b58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " epoch 1 | 500/8313 batches | loss 0.012067549609153452 | accuracy 0.7594186626746507\n",
            " epoch 1 | 1000/8313 batches | loss 0.011983260302548553 | accuracy 0.7614572927072927\n",
            " epoch 1 | 1500/8313 batches | loss 0.011996481639595527 | accuracy 0.7619711858760826\n",
            " epoch 1 | 2000/8313 batches | loss 0.011977427110340314 | accuracy 0.7617285107446277\n",
            " epoch 1 | 2500/8313 batches | loss 0.011935190605621536 | accuracy 0.7622263594562175\n",
            " epoch 1 | 3000/8313 batches | loss 0.011899726866610703 | accuracy 0.7627665778073975\n",
            " epoch 1 | 3500/8313 batches | loss 0.011863212522797094 | accuracy 0.7633354755784062\n",
            " epoch 1 | 4000/8313 batches | loss 0.011827568239603407 | accuracy 0.7642620594851287\n",
            " epoch 1 | 4500/8313 batches | loss 0.011790084726885012 | accuracy 0.7647363085980893\n",
            " epoch 1 | 5000/8313 batches | loss 0.011744137418543344 | accuracy 0.7655718856228754\n",
            " epoch 1 | 5500/8313 batches | loss 0.011713607635784476 | accuracy 0.7662669287402291\n",
            " epoch 1 | 6000/8313 batches | loss 0.01167808960871192 | accuracy 0.7667784535910681\n",
            " epoch 1 | 6500/8313 batches | loss 0.011640571527898774 | accuracy 0.767321854330103\n",
            " epoch 1 | 7000/8313 batches | loss 0.011606113319579869 | accuracy 0.767901460505642\n",
            " epoch 1 | 7500/8313 batches | loss 0.01157810007305308 | accuracy 0.7684225436608452\n",
            " epoch 1 | 8000/8313 batches | loss 0.011541213943435484 | accuracy 0.7690542588426447\n",
            " epoch 2 | 500/8313 batches | loss 0.010877661663525832 | accuracy 0.7810940618762475\n",
            " epoch 2 | 1000/8313 batches | loss 0.010908567269875125 | accuracy 0.7804539210789211\n",
            " epoch 2 | 1500/8313 batches | loss 0.010781582535494574 | accuracy 0.7831133411059293\n",
            " epoch 2 | 2000/8313 batches | loss 0.010783403229409608 | accuracy 0.7834598325837081\n",
            " epoch 2 | 2500/8313 batches | loss 0.010724713305299614 | accuracy 0.7843550079968012\n",
            " epoch 2 | 3000/8313 batches | loss 0.010660055597300025 | accuracy 0.7855766827724092\n",
            " epoch 2 | 3500/8313 batches | loss 0.010645863662988067 | accuracy 0.7858826049700086\n",
            " epoch 2 | 4000/8313 batches | loss 0.010604162465374087 | accuracy 0.7868267308172957\n",
            " epoch 2 | 4500/8313 batches | loss 0.010582460605631873 | accuracy 0.7873007387247278\n",
            " epoch 2 | 5000/8313 batches | loss 0.010553749151366981 | accuracy 0.7877643221355729\n",
            " epoch 2 | 5500/8313 batches | loss 0.0105344397977497 | accuracy 0.7880243364842756\n",
            " epoch 2 | 6000/8313 batches | loss 0.010499832578099007 | accuracy 0.788790409931678\n",
            " epoch 2 | 6500/8313 batches | loss 0.010473534412045417 | accuracy 0.7894530649130903\n",
            " epoch 2 | 7000/8313 batches | loss 0.01045334611807448 | accuracy 0.7898693043850878\n",
            " epoch 2 | 7500/8313 batches | loss 0.01043551887403302 | accuracy 0.790363368217571\n",
            " epoch 2 | 8000/8313 batches | loss 0.010409614688988857 | accuracy 0.790854268216473\n",
            " epoch 3 | 500/8313 batches | loss 0.009819739764024636 | accuracy 0.8041417165668663\n",
            " epoch 3 | 1000/8313 batches | loss 0.009767472914249807 | accuracy 0.8044455544455544\n",
            " epoch 3 | 1500/8313 batches | loss 0.009804344372135116 | accuracy 0.8041930379746836\n",
            " epoch 3 | 2000/8313 batches | loss 0.00977702987036374 | accuracy 0.8050427911044478\n",
            " epoch 3 | 2500/8313 batches | loss 0.00976278751035456 | accuracy 0.8050279888044782\n",
            " epoch 3 | 3000/8313 batches | loss 0.009763074554664339 | accuracy 0.8049660529823393\n",
            " epoch 3 | 3500/8313 batches | loss 0.009741142185729969 | accuracy 0.805046772350757\n",
            " epoch 3 | 4000/8313 batches | loss 0.009696216484142098 | accuracy 0.8058258872781805\n",
            " epoch 3 | 4500/8313 batches | loss 0.009701279296109963 | accuracy 0.8058730004443457\n",
            " epoch 3 | 5000/8313 batches | loss 0.009687109817990053 | accuracy 0.80625124975005\n",
            " epoch 3 | 5500/8313 batches | loss 0.009650025910033711 | accuracy 0.8068021723323032\n",
            " epoch 3 | 6000/8313 batches | loss 0.00962338222222087 | accuracy 0.8073133644392602\n",
            " epoch 3 | 6500/8313 batches | loss 0.009616257989292895 | accuracy 0.807517593447162\n",
            " epoch 3 | 7000/8313 batches | loss 0.009609161753424545 | accuracy 0.8077015783459506\n",
            " epoch 3 | 7500/8313 batches | loss 0.009591391782213384 | accuracy 0.8079943507532329\n",
            " epoch 3 | 8000/8313 batches | loss 0.009572290776011337 | accuracy 0.8084419135108112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, test_loss = evaluate(test_dataloader)\n",
        "\n",
        "print(f\"The Accuracy in the test dataset is: {test_acc}\")\n",
        "print(f\"The Loss in the test dataset is: {test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqn8TDqObGA7",
        "outputId": "28271021-5689-49ac-93db-e1e7d87829e7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy in the test dataset is: 0.8185428571428571\n",
            "The Loss in the test dataset is: 0.009112035138692175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-9X1pMx3Zibd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}